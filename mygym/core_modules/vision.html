

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Vision &mdash; mygym 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/mygym_theme.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.png"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Camera" href="../other/camera.html" />
    <link rel="prev" title="Reward" href="reward.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> mygym
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">How to</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/installation.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/visualization.html">Visualize workspace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/basic_training.html">Train a robot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/tutorial_parametric.html">Set parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/tutorial_config.html">Edit config file</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/train_camera.html">Train robot - supervised vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/train_vae.html">Train a robot - unsupervised vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/parallel_training.html">Train in parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/tensorboard.html">Evaluate with tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/test_model.html">Test trained model</a></li>
</ul>
<p class="caption"><span class="caption-text">How to - advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/write_reward.html">Create custom reward</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/create_workspace.html">Create custom workspace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/create_network.html">Create custom network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/dataset.html">Generate dataset</a></li>
</ul>
<p class="caption"><span class="caption-text">Gym</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../environments/workspace.html">Workspaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/mygym_objects.html">Objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/mygym_robots.html">Robots</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/gym_env.html">GymEnv</a></li>
</ul>
<p class="caption"><span class="caption-text">Baselines</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../baselines/table.html">Baseline networks</a></li>
</ul>
<p class="caption"><span class="caption-text">Important classes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="base_env.html">BaseEnv</a></li>
<li class="toctree-l1"><a class="reference internal" href="robot.html">Robot</a></li>
<li class="toctree-l1"><a class="reference internal" href="task.html">Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="reward.html">Reward</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#yolact">YOLACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vae">VAE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#how-to-train-a-custom-vae">How to train a custom VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generating-a-dataset">Generating a dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-vae">Training VAE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../other/camera.html">Camera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../other/env_object.html">Object</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">mygym</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Vision</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/core_modules/vision.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vision">
<span id="id1"></span><h1>Vision<a class="headerlink" href="#vision" title="Permalink to this headline">¶</a></h1>
<p>MyGym enables you to use pre-trained vision models to extend the versatility of your training scenarios.
The vision models can be used instead of ground truth data from simulator to retrieve information about
the environment where robot performs its task. Vision models take simulator’s camera data (RGB and/or depth image)
as inputs to inference and return information about observed scene. Thanks to that, your training becomes
independent on ground truth from simulator and can be therefore easier transfered to real robot tasks.</p>
<p>MyGym integrated two different vision modules - YOLACT and VAE - and you can alternate between ground truth
and these when specifying the type of source of reward signal in <em>config file</em> or as a command line argument:
<strong>reward_type=</strong> either <strong>gt</strong> (ground truth) or <strong>3dvs</strong> (YOLACT) or <strong>2dvu</strong> (VAE).</p>
<div class="section" id="yolact">
<h2>YOLACT<a class="headerlink" href="#yolact" title="Permalink to this headline">¶</a></h2>
<p>Mygym implements YOLACT <a class="footnote-reference brackets" href="#id3" id="id2">1</a> for instance segmantation. If <strong>3dvs</strong> is chosen for <strong>reward_type</strong>, the pre-trained YOLACT
model is used to get observations from the environment. The input into YOLACT inference is RGB image rendered by
the active camera, the inference results are masks and bounding boxes of detected objects. The vision module further
calculates the position of centroids of detected objects in pixel space. Lastly, the vision module utilizes the depth
image from the active camera to project the object’s centroid into 3D worl coordinates. This way, the absolute
position of task objects is obtained only from sensory data without any ground truth inputs.</p>
<p>The current pre-trained model can detect all <a class="reference internal" href="../environments/mygym_objects.html#mygym-objects"><span class="std std-ref">Objects</span></a> and three of <a class="reference internal" href="../environments/mygym_robots.html#mygym-robots"><span class="std std-ref">Robots</span></a> including
their grippers (kuka, jaco, panda).</p>
<p>If you would like to train new YOLACT model, you can use prepared dataset generator available in myGym,
see <a class="reference internal" href="../user_guide/dataset.html#dataset"><span class="std std-ref">Generate dataset</span></a>. For instructions regarding training itself, visit <a class="reference external" href="https://github.com/dbolya/yolact">YOLACT</a> home page.</p>
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Daniel Bolya, Chong Zhou, Fanyi Xiao, &amp; Yong Jae Lee (2019). YOLACT: Real-time Instance Segmentation. In ICCV.</p>
</dd>
</dl>
</div>
<div class="section" id="vae">
<h2>VAE<a class="headerlink" href="#vae" title="Permalink to this headline">¶</a></h2>
<p>The objective of an unsupervised version of the prepared tasks (reach
task, push task, pick and place etc.) is to minimize the difference
between the actual and goal scene images. To measure their difference,
we have implemented a variational autoencoder (VAE) that
compresses each image into an n-dimensional latent vector. Since the VAE
is optimized so that it preserves similarities among images also in the
latent space (scenes with objects close to each other will have their
encoded vectors also closer to each other), it is possible to measure
the euclidean distance between the encoded scenes and use it for reward
calculation - i.e., the smaller the euclidean distance between actual
and goal image, the higher the reward. Please note that the limitation of
using VAE is that it works conveniently only with 2D information - i.e.,
it is a very weak source of visual information in 3D tasks such as pick
and place.</p>
<p>We provide a pretrained VAE for some of the task scenarios, but we also
include code for training of your own VAE (including dataset
generation), so that you can create custom experiments. To learn how to train your robot with the pretrained weights, see <a class="reference internal" href="../user_guide/train_vae.html#train-vae"><span class="std std-ref">Train a robot - unsupervised vision</span></a></p>
<div class="section" id="how-to-train-a-custom-vae">
<h3>How to train a custom VAE<a class="headerlink" href="#how-to-train-a-custom-vae" title="Permalink to this headline">¶</a></h3>
<p>You are free to train your own VAE with a custom set of objects, level
of randomisation, background scene or type of robot. Here we describe
how.</p>
</div>
<div class="section" id="generating-a-dataset">
<h3>Generating a dataset<a class="headerlink" href="#generating-a-dataset" title="Permalink to this headline">¶</a></h3>
<p>To generate a VAE dataset, run the following script:</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">generate_dataset.py</span> <span class="pre">configs/dataset_vae.json</span></code></p>
<p>All the dataset parameters shall be adjusted in
configs/dataset_vae.json. They are described in comments, so here we
highlight the most important ones:</p>
<ul class="simple">
<li><p><strong>output_folder</strong>: where to save the resulting dataset</p></li>
<li><p><strong>imsize</strong>: the resulting square image size, that will be saved. We
currently only support VAE architectures for imsize of 128 or 64. The
cropping of the image is done atomatically and can be adjusted in the
code.</p></li>
<li><p><strong>num_episodes</strong>: corresponds to the overall number of images in the
dataset (in case the make_shot_every_frame parameter is set to 1)</p></li>
<li><p><strong>random_arm_movement</strong>: whether to move the robot randomly,
otherwise it stays fixed in its default position</p></li>
<li><p><strong>used_class_names_quantity</strong>: what kind of objects do you want to
show in the scene and how often. The names correspond to the urdf
object names in the envs/objects directory. The first number in each
list corresponds to the frequency, i.e. 1 is a default frequency and
values above 1 make the object appear more ofthen than the others.</p></li>
<li><p><strong>object_sampling_area</strong>: set the area in which the selected objects
will be sampled; the format is <em>xxyyzz</em></p></li>
<li><p><strong>num_objects_range</strong>: in each image taken, a random number of
objects from this range will appear in the scene</p></li>
<li><p><strong>object_colors</strong>: if you have a color randomizer enabled and want
some objects to have fixed color, you can set it up here</p></li>
<li><p><strong>active_camera</strong>: the viewpoint from which the scene will be
captured. The number 1 defines the active camera that will be used.
We currently only support one camera viewpoint for dataset
generation.</p></li>
</ul>
</div>
<div class="section" id="training-vae">
<h3>Training VAE<a class="headerlink" href="#training-vae" title="Permalink to this headline">¶</a></h3>
<p>Once you have your dataset ready, you can continue with VAE training.
This is handled with the following script:</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">train_vae.py</span> <span class="pre">--config</span> <span class="pre">vae/config.ini</span> <span class="pre">--offscreen</span></code></p>
<p>The –offscreen parameter turns off any kind of visualisation, so if you
want to see the progress, do not use it. Otherwise, all parameters can
be set in the config.ini file as follows:</p>
<ul class="simple">
<li><p><strong>n_latents</strong>: the dimensionality of the latent vector z</p></li>
<li><p><strong>batch_size</strong>: choose any integer</p></li>
<li><p><strong>lr</strong>: the learning rate</p></li>
<li><p><strong>beta</strong>: size of the beta paremeter to induce disentanglement of the
latent space as proposed in <a class="reference external" href="https://openreview.net/forum?id=Sy2fzU9gl">this
paper</a></p></li>
<li><p><strong>img_size</strong>: size of the square images to train on. Currently the
only supported sizes are 64 or 128</p></li>
<li><p><strong>use_cuda</strong></p></li>
<li><p><strong>n_epochs</strong>: the number of training epochs</p></li>
<li><p><strong>viz_every_n_epochs</strong>: how often to save the image reconstruction to
monitor the training progress</p></li>
<li><p><strong>annealing_epochs</strong>: the number of epochs for which to gradually
increase the impact of KLD term in the ELBO loss. See <a class="reference external" href="https://arxiv.org/abs/1903.10145">this
paper</a> for more info</p></li>
<li><p><strong>log_interval</strong>: how often to print out the log about the training
progress in the console</p></li>
<li><p><strong>test_data_percentage</strong>: the fraction of the training dataset that
will be used as testing data</p></li>
<li><p><strong>dataset_path</strong>: path to the dataset folder containing images</p></li>
</ul>
<p>The trained VAE will be saved in the ciircgym/vae/trained_models/ folder,
along with the config used for the training and visualisations.</p>
<span class="target" id="module-myGym.envs.vision_module"></span><dl class="py class">
<dt id="myGym.envs.vision_module.VisionModule">
<em class="property">class </em><code class="sig-prename descclassname">myGym.envs.vision_module.</code><code class="sig-name descname">VisionModule</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">vision_src</span><span class="o">=</span><span class="default_value">'ground_truth'</span></em>, <em class="sig-param"><span class="n">env</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">vae_path</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">yolact_path</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">yolact_config</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Vision class that retrieves information from environment based on a visual subsystem (YOLACT, VAE) or ground truth</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="field-list simple">
<dt class="field-odd">param vision_src</dt>
<dd class="field-odd"><p>(string) Source of information from environment (ground_truth, yolact, vae)</p>
</dd>
<dt class="field-even">param env</dt>
<dd class="field-even"><p>(object) Environment, where the training takes place</p>
</dd>
<dt class="field-odd">param vae_path</dt>
<dd class="field-odd"><p>(string) Path to a trained VAE in 2dvu reward type</p>
</dd>
<dt class="field-even">param yolact_path</dt>
<dd class="field-even"><p>(string) Path to a trained Yolact in 3dvu reward type</p>
</dd>
<dt class="field-odd">param yolact_config</dt>
<dd class="field-odd"><p>(string) Path to saved Yolact config obj or name of an existing one in the data/Config script or None for autodetection</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.get_module_type">
<code class="sig-name descname">get_module_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.get_module_type"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.get_module_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Get source of the information from environment (ground_truth, yolact, vae)</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return source</dt>
<dd class="field-odd"><p>(string) Source of information</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.crop_image">
<code class="sig-name descname">crop_image</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">img</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.crop_image"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.crop_image" title="Permalink to this definition">¶</a></dt>
<dd><p>Crop image by 1/4 from each side</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="field-list simple">
<dt class="field-odd">param img</dt>
<dd class="field-odd"><p>(list) Original image</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return img</dt>
<dd class="field-odd"><p>(list) Cropped image</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.get_obj_pixel_position">
<code class="sig-name descname">get_obj_pixel_position</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obj</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">img</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.get_obj_pixel_position"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.get_obj_pixel_position" title="Permalink to this definition">¶</a></dt>
<dd><p>Get mask and centroid in pixel space coordinates of an object from 2D image</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="field-list simple">
<dt class="field-odd">param obj</dt>
<dd class="field-odd"><p>(object) Object to find its mask and centroid</p>
</dd>
<dt class="field-even">param img</dt>
<dd class="field-even"><p>(array) 2D input image to inference of vision model</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return mask</dt>
<dd class="field-odd"><p>(list) Mask of object</p>
</dd>
<dt class="field-even">return centroid</dt>
<dd class="field-even"><p>(list) Centroid of object in pixel sprace coordinates</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.get_obj_bbox">
<code class="sig-name descname">get_obj_bbox</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obj</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">img</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.get_obj_bbox"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.get_obj_bbox" title="Permalink to this definition">¶</a></dt>
<dd><p>Get bounding box of an object from 2D image</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="field-list simple">
<dt class="field-odd">param obj</dt>
<dd class="field-odd"><p>(object) Object to find its bounding box</p>
</dd>
<dt class="field-even">param img</dt>
<dd class="field-even"><p>(array) 2D input image to inference of vision model</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return bbox</dt>
<dd class="field-odd"><p>(list) Bounding box of object</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.get_obj_position">
<code class="sig-name descname">get_obj_position</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obj</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">img</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">depth</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.get_obj_position"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.get_obj_position" title="Permalink to this definition">¶</a></dt>
<dd><p>Get object position in world coordinates of environment from 2D and depth image</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="field-list simple">
<dt class="field-odd">param obj</dt>
<dd class="field-odd"><p>(object) Object to find its mask and centroid</p>
</dd>
<dt class="field-even">param img</dt>
<dd class="field-even"><p>(array) 2D input image to inference of vision model</p>
</dd>
<dt class="field-odd">param depth</dt>
<dd class="field-odd"><p>(array) Depth input image to inference of vision model</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return position</dt>
<dd class="field-odd"><p>(list) Centroid of object in world coordinates</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.get_obj_orientation">
<code class="sig-name descname">get_obj_orientation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obj</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">img</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.get_obj_orientation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.get_obj_orientation" title="Permalink to this definition">¶</a></dt>
<dd><p>Get object orientation in world coordinates of environment from 2D image</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="field-list simple">
<dt class="field-odd">param obj</dt>
<dd class="field-odd"><p>(object) Object to find its mask and centroid</p>
</dd>
<dt class="field-even">param img</dt>
<dd class="field-even"><p>(array) 2D input image to inference of vision model</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return orientation</dt>
<dd class="field-odd"><p>(list) Orientation of object in world coordinates</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.vae_generate_sample">
<code class="sig-name descname">vae_generate_sample</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.vae_generate_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.vae_generate_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate image as a sample of VAE latent representation</p>
<dl class="simple">
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return dec_img</dt>
<dd class="field-odd"><p>Generated image from VAE latent representation</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.encode_with_vae">
<code class="sig-name descname">encode_with_vae</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">imgs</span></em>, <em class="sig-param"><span class="n">task</span><span class="o">=</span><span class="default_value">'reach'</span></em>, <em class="sig-param"><span class="n">decode</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.encode_with_vae"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.encode_with_vae" title="Permalink to this definition">¶</a></dt>
<dd><p>Encode the input image into an n-dimensional latent variable using VAE model</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="field-list simple">
<dt class="field-odd">param imgs</dt>
<dd class="field-odd"><p>(list of arrays) Input images</p>
</dd>
<dt class="field-even">param task</dt>
<dd class="field-even"><p>(string) Type of learned task (reach, push, …)</p>
</dd>
<dt class="field-odd">param decode</dt>
<dd class="field-odd"><p>(bool) Whether to decode encoded images from latent representation back to image array</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return latent_z</dt>
<dd class="field-odd"><p>(list) Latent representation of images</p>
</dd>
<dt class="field-even">return dec_img</dt>
<dd class="field-even"><p>(list of arrays) Decoded images from latent representation back to image arrays</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="myGym.envs.vision_module.VisionModule.inference_yolact">
<code class="sig-name descname">inference_yolact</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">img</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/myGym/envs/vision_module.html#VisionModule.inference_yolact"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#myGym.envs.vision_module.VisionModule.inference_yolact" title="Permalink to this definition">¶</a></dt>
<dd><p>Infere using YOLACT model</p>
<dl class="simple">
<dt>Parameters:</dt><dd><dl class="field-list simple">
<dt class="field-odd">param img</dt>
<dd class="field-odd"><p>(array) Input 2D image</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return classes</dt>
<dd class="field-odd"><p>(list of ints) Classes IDs of detected objects</p>
</dd>
<dt class="field-even">return class_names</dt>
<dd class="field-even"><p>(list of strings) Classes names of detected objects</p>
</dd>
<dt class="field-odd">return scores</dt>
<dd class="field-odd"><p>(list of floats) Scores (confidence) of object detections</p>
</dd>
<dt class="field-even">return boxes</dt>
<dd class="field-even"><p>(list of lists) Bounding boxes of detected objects</p>
</dd>
<dt class="field-odd">return masks</dt>
<dd class="field-odd"><p>(list of lists) Masks of detected objects</p>
</dd>
<dt class="field-even">return centroids</dt>
<dd class="field-even"><p>(list of lists) Centroids of detected objects in pixel space coordinates</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../other/camera.html" class="btn btn-neutral float-right" title="Camera" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="reward.html" class="btn btn-neutral float-left" title="Reward" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Nikita Sokovnin.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>